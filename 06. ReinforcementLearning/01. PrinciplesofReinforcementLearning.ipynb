{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Principles of Reinforcement Learning (강화 학습 원리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 개요\n",
    "- 강화 학습은 이전까지 학습한 다른 인공지능 네트워크, 학습 모델과는 근본적인 차이를 가지고 있음\n",
    "- 이전까지의 인공지능은 지도 학습이라고 하는 큰 하나의 카테고리로 묶여질 수 있음\n",
    "- 지도 학습은 문제에 대한 정답이 제공되는 학습 방식인데, 이것은 주어진 이미지에 적힌 글자가 무엇인지, 심전도 데이터가 정상인지 비정상인지 등에 대한 정보가 주어진다는 뜻\n",
    "- GAN에서도 판별자에 입력되는 데이터가 원본인지 생성된 데이터인지를 네트워크에 알려주며, 판별자와 생성자는 실제 출력과 정답 데이터의 차이로부터 계산된 손실 값을 바탕으로 학습됨\n",
    "- 강화 학습은 특정 입력에 대해 지정된 정답을 알 수 없는 경우를 위해 개발된 학습법으로, 강화 학습의 목표는 주어진 상황에서 최적의 행동을 도출하는 것\n",
    "- 강화 학습은 복잡한 상황에 대한 대응이 실제로 좋은 결과를 가져올 것인지에 대해서도 확신하기 어려움\n",
    "- 또한 이런 문제를 행동에 따라 변화하는 환경을 구현하여 해결\n",
    "- 강화 학습을 사용하면 각 상황에 대한 정답을 명확히 알지 못하는 경우에도, 목표만 명확히 주어진다면 그 상황에서 수행해야 할 행동을 도출하는 인공지능을 학습시킬 수 있음\n",
    "- 강화 학습은 주어진 환경에서의 최적 행동을 추정하는데에 사용되며, 최적 행동을 일일히 정의하기는 어렵지만 행동의 목표가 명확한 경우에 활용될 수 있는 인공지능 학습 알고리즘임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 환경과 에이전트\n",
    "- 강화 학습은 환경(State)과 에이전트(Agent)의 상호작용을 기반으로 함\n",
    "- 여기서 환경은 학습이 이루어지는 다양한 정보들을, 에이전트는 주어진 환경을 기반으로 어떤 행동을 할지 제시하는 인공지능을 의미\n",
    "- 환경은 에이전트가 제시한 행동에 의해 변화됨\n",
    "- 환경은 에이전트와 무관하게 시간의 흐름에 따라 변화하기도 함\n",
    "- 에이전트에 의해 변화된 환경은 다시 에이전트에게 제시됨\n",
    "- 에이전트는 이 정보를 바탕으로 새로운 행동을 제시하고, 환경은 이를 다시 반영\n",
    "- 환경은 그 종류에 따라 에이전트와 동기화되어 운영될 수도, 비동기식으로 운영될 수도 있음\n",
    "- 동기화된다는 것은 에이전트가 어떤 행동(Action)을 취하지 않으면 환경의 변화가 일어나지 않는다는 뜻\n",
    "- 바둑이나 장기 등의 보드게임이 대표적인 예인데, 이 경우 환경과 에이전트와의 관계는 일종의 '턴'제로 동작한다고 볼 수 있음\n",
    "- 에이전트의 행동 제시가 없다면 환경은 변화하지 않기 때문\n",
    "- 반면, 실시간 전략 게임(Real Time Strategy) 등에서는 에이전트 없이도 상황이 지속적으로 바뀜\n",
    "- 이 경우는 게임 내 시간을 기준으로 환경의 변화가 이루어짐\n",
    "- 환경은 지속적으로 변화하며, 해당 시간에 에이전트는 특정한 행동을 취할 수도, 아무런 행동을 취하지 않을 수도(No Action) 있음\n",
    "- 여기서 에이전트가 취할 수 있는 행동들의 집합을 액션 스페이스(Action Space)라고 부름\n",
    "- 게임을 사람이 플레이하는 경우에는 이 행동을 결정하는 것은 사람이지만, 강화 학습에서는 행동의 결정을 사람 대신인공지능 에이전트(Agent)가 하게 됨\n",
    "- 강화 학습은 인공신경망 외에도 다양한 인공지능을 포괄하는 개념으로 에이전트는 다양한 이론을 탑재한 모델이 될 수 있으나, 여기에서는 인공신경망을 사용하여 구성된 에이전트가 사용하는 경우에 한정하여 설명\n",
    "- 이 설명이 조금 어렵다면, 에이전트는 인공지능 플레이어, 환경은 게임 그 자체에 빗대어 이해하면 됨\n",
    "- 사람의 눈으로 게임 환경을 보고 버튼을 눌러 캐릭터를 조작하듯이, 인공지능 플레이어가 환경을 데이터로 받아들여 수행할 명령을 지시하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. 학습\n",
    "- 강화 학습의 목표는 똑똑한 에이전트를 만드는 것\n",
    "- 딥 네트워크로 구성된 에이전트를 일반적인 방식으로 학습시키려면 특정 입력에 대한 목표 출력(타깃)을 지정해 주어야 하지만, 이런 학습용 데이터셋을 구성하기는 매우 어렵고, 때에 따라서는 불가능에 가까운 경우도 많음\n",
    "- 경우의 수가 너무 다양하고 정답을 모르는 경우도 많기 때문 (ex) 다수의 적 출현, 난이도 상승 등)\n",
    "- 일반적으로 강화 학습에서는 이 문제를 시뮬레이션과 보상 함수를 사용하여 해결\n",
    "- 시뮬레이션은 임의의 환경 상태에서 시작하여, 이 환경 상태를 에이전트에게 제공하고 에이전트의 응답을 환경에 적용하는 과정\n",
    "- 이것은 앞서 설명한 환경과 에이전트의 관계를 이용하여 게임을 플레이하는 과정과 같지만, 처음 주어지는 환경 상태는 임의의 데이터이며, 시뮬레이션이 끝난 뒤에는 보상이 주어진다는 점이 다름\n",
    "- 시뮬레이션이 수행되는 동안 에이전트 내부 네트워크의 가중치는 수정되지 않으며 동일한 상태로 유지된다는 점에 주의해야 함\n",
    "- 시뮬레이션은 특정 시간 동안 계속해서 수행되거나, 특정 조건을 만족하면(동전 획득, 지하로 추락, 적과의 부딪힘 등) 종료\n",
    "- 시뮬레이션이 종료되면 에이전트에게 보상이 주어지는데, 이 보상은 게임의 목표를 얼마나 잘 달성하였는지에 따라 주어지며, 다양한 방법으로 정의될 수 있음\n",
    "- 예로 든 게임의 목표가 동전을 빨리 획득하는 것이라고 한다면, 보상의 양은 동전을 획득한 시간에 반비례하여 정의되게 됨\n",
    "- 예를 들어 '보상 = 1 / 걸린 시간'과 같이 정의할 수 있음\n",
    "- 이와 같이 보상을 정의하는 함수를 보상 함수(Reward Function)라 함\n",
    "- 강화 학습에서의 에이전트 학습은 여러 번의 시뮬레이션을 통해 보상을 최대화하도록 학습될 수 있음\n",
    "- 딥러닝 분류 네트워크의 학습 시에 에러를 최소화하는 방향으로 가중치를 학습하는 것을 생각하면, 강화 학습의 학습은 이와 반대라고 할 수 있음\n",
    "- 강화 학습의 학습은 보상 함수를 통해 그 방향이 정해져 수행되므로, 보상 함수를 잘 정의하는 것은 강화 학습의 구성에서 매우 중요한 요소\n",
    "- 보상은 모든 환경 상태(S)에서 계산될 수 있도록 정의하는 편이 학습에 용이\n",
    "- 에이전트의 성능을 더욱 세밀하게 정의할 수 있게 되어, 네트워크를 학습해 나가야 하는 방향을 알아내기가 상대적으로 쉬워짐\n",
    "- 한 시뮬레이션에 대한 보상을 정의할 때는, 보통 할인율(Discount Factor)이라는 개념을 사용하여 더 빨리 얻은 보상에 대한 가치를 더 높게 책정\n",
    "- 즉, 시뮬레이션 시작으로부터 n 시간이 지난 특정 상태의 가치는 해당 생태의 기본 가치에 할인율을 n 거듭제곱한 수를 곱하여 계산됨\n",
    "- 어떤 상태에 대한 기본 가치의 크기가 100이고, 시간이 2만큼 지난 상태에 할인율이 0.9라면, 그 가치는 81(=100 * 0.9 * 0.9)이 되는 것"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

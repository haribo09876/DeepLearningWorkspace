{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Optimization of deep learning parameters (딥러닝 파라미터의 최적화)\n",
    "- 완전 연결층, 합성곱 신경망, LSTM층을 설계하고 구현하기 위해서는 다양한 파라미터의 값을 지정해야 함\n",
    "1) 데이터 준비\n",
    "2) 딥너링 모델 생성\n",
    "3) 학습\n",
    "4) 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "data_no = pd.read_csv(\"ptbdb_normal.csv\")\n",
    "data_ab = pd.read_csv(\"ptbdb_abnormal.csv\")\n",
    "data_no = np.array(data_no)\n",
    "data_ab = np.array(data_ab)\n",
    "\n",
    "nTrain = 3000\n",
    "nTest = 1000\n",
    "X_train = np.concatenate((data_no[:nTrain, :], data_ab[:nTrain, :]), 0)\n",
    "y_train = np.concatenate((np.zeros(nTrain, ), np.ones(nTrain, )), 0)\n",
    "X_test = np.concatenate((data_no[nTrain:nTrain+nTest, :], data_ab[nTrain:nTrain+nTest, :]), 0)\n",
    "y_test = np.concatenate((np.zeros(nTest, ), np.ones(nTest, )), 0)\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모델 생성\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3, input_shape=(X_train.shape[1], 1), activation=\"relu\"))\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=3, strides=2))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, input_shape=(X_train.shape[1], 1), activation=\"relu\"))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling1D(pool_size=3, strides=2))\n",
    "model.add(layers.LSTM(16))\n",
    "model.add(layers.Dense(units=2, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(learning_rate=0.01), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "38/38 [==============================] - 5s 37ms/step - loss: 0.6174 - accuracy: 0.6623 - val_loss: 0.8379 - val_accuracy: 0.5308\n",
      "Epoch 2/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.5809 - accuracy: 0.7104 - val_loss: 0.8550 - val_accuracy: 0.4725\n",
      "Epoch 3/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5663 - accuracy: 0.7198 - val_loss: 0.8082 - val_accuracy: 0.5125\n",
      "Epoch 4/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5591 - accuracy: 0.7185 - val_loss: 0.6801 - val_accuracy: 0.6300\n",
      "Epoch 5/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5642 - accuracy: 0.7265 - val_loss: 0.7263 - val_accuracy: 0.5892\n",
      "Epoch 6/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5548 - accuracy: 0.7240 - val_loss: 0.7371 - val_accuracy: 0.5350\n",
      "Epoch 7/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5389 - accuracy: 0.7406 - val_loss: 0.9751 - val_accuracy: 0.4142\n",
      "Epoch 8/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5514 - accuracy: 0.7308 - val_loss: 0.8972 - val_accuracy: 0.4017\n",
      "Epoch 9/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.5197 - accuracy: 0.7521 - val_loss: 0.7746 - val_accuracy: 0.5075\n",
      "Epoch 10/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.5830 - accuracy: 0.7219 - val_loss: 0.7699 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.5143 - accuracy: 0.7473 - val_loss: 0.8021 - val_accuracy: 0.4992\n",
      "Epoch 12/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.4556 - accuracy: 0.7823 - val_loss: 0.7315 - val_accuracy: 0.5692\n",
      "Epoch 13/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.4108 - accuracy: 0.8148 - val_loss: 0.6687 - val_accuracy: 0.6342\n",
      "Epoch 14/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.3961 - accuracy: 0.8190 - val_loss: 0.4512 - val_accuracy: 0.7100\n",
      "Epoch 15/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.3648 - accuracy: 0.8375 - val_loss: 0.6431 - val_accuracy: 0.6808\n",
      "Epoch 16/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.3273 - accuracy: 0.8612 - val_loss: 0.3609 - val_accuracy: 0.8342\n",
      "Epoch 17/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.3768 - accuracy: 0.8283 - val_loss: 0.2666 - val_accuracy: 0.8717\n",
      "Epoch 18/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.3065 - accuracy: 0.8758 - val_loss: 0.3858 - val_accuracy: 0.8358\n",
      "Epoch 19/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.2322 - accuracy: 0.9123 - val_loss: 0.3041 - val_accuracy: 0.8667\n",
      "Epoch 20/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.2312 - accuracy: 0.9102 - val_loss: 0.2393 - val_accuracy: 0.8958\n",
      "Epoch 21/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1922 - accuracy: 0.9287 - val_loss: 0.2337 - val_accuracy: 0.9050\n",
      "Epoch 22/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1813 - accuracy: 0.9267 - val_loss: 0.4078 - val_accuracy: 0.8225\n",
      "Epoch 23/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.2030 - accuracy: 0.9223 - val_loss: 0.1791 - val_accuracy: 0.9275\n",
      "Epoch 24/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.1445 - accuracy: 0.9460 - val_loss: 0.4961 - val_accuracy: 0.8108\n",
      "Epoch 25/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1683 - accuracy: 0.9369 - val_loss: 0.2242 - val_accuracy: 0.9050\n",
      "Epoch 26/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1389 - accuracy: 0.9488 - val_loss: 0.0831 - val_accuracy: 0.9767\n",
      "Epoch 27/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1329 - accuracy: 0.9538 - val_loss: 0.1619 - val_accuracy: 0.9375\n",
      "Epoch 28/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1371 - accuracy: 0.9483 - val_loss: 0.0936 - val_accuracy: 0.9675\n",
      "Epoch 29/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1443 - accuracy: 0.9469 - val_loss: 0.2393 - val_accuracy: 0.8925\n",
      "Epoch 30/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1142 - accuracy: 0.9579 - val_loss: 0.2263 - val_accuracy: 0.9117\n",
      "Epoch 31/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1057 - accuracy: 0.9615 - val_loss: 0.3339 - val_accuracy: 0.8767\n",
      "Epoch 32/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.1200 - accuracy: 0.9558 - val_loss: 0.0790 - val_accuracy: 0.9700\n",
      "Epoch 33/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0899 - accuracy: 0.9673 - val_loss: 0.1468 - val_accuracy: 0.9492\n",
      "Epoch 34/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0943 - accuracy: 0.9671 - val_loss: 0.2780 - val_accuracy: 0.9083\n",
      "Epoch 35/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.1151 - accuracy: 0.9581 - val_loss: 0.0832 - val_accuracy: 0.9692\n",
      "Epoch 36/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0998 - accuracy: 0.9629 - val_loss: 0.1111 - val_accuracy: 0.9567\n",
      "Epoch 37/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0992 - accuracy: 0.9642 - val_loss: 0.1016 - val_accuracy: 0.9600\n",
      "Epoch 38/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.1028 - accuracy: 0.9652 - val_loss: 0.4942 - val_accuracy: 0.8300\n",
      "Epoch 39/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.1173 - accuracy: 0.9594 - val_loss: 0.2510 - val_accuracy: 0.9142\n",
      "Epoch 40/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.1016 - accuracy: 0.9625 - val_loss: 0.0945 - val_accuracy: 0.9667\n",
      "Epoch 41/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0797 - accuracy: 0.9737 - val_loss: 0.1056 - val_accuracy: 0.9617\n",
      "Epoch 42/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0636 - accuracy: 0.9783 - val_loss: 0.2199 - val_accuracy: 0.9292\n",
      "Epoch 43/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0646 - accuracy: 0.9773 - val_loss: 0.1532 - val_accuracy: 0.9467\n",
      "Epoch 44/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0908 - accuracy: 0.9681 - val_loss: 0.1973 - val_accuracy: 0.9308\n",
      "Epoch 45/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0687 - accuracy: 0.9762 - val_loss: 0.1658 - val_accuracy: 0.9433\n",
      "Epoch 46/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0870 - accuracy: 0.9675 - val_loss: 0.1496 - val_accuracy: 0.9475\n",
      "Epoch 47/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0799 - accuracy: 0.9690 - val_loss: 0.1172 - val_accuracy: 0.9525\n",
      "Epoch 48/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0666 - accuracy: 0.9783 - val_loss: 0.1351 - val_accuracy: 0.9533\n",
      "Epoch 49/50\n",
      "38/38 [==============================] - 1s 19ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0624 - val_accuracy: 0.9775\n",
      "Epoch 50/50\n",
      "38/38 [==============================] - 1s 18ms/step - loss: 0.0545 - accuracy: 0.9819 - val_loss: 0.3123 - val_accuracy: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x10ab0e76710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9539999999999503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증\n",
    "o = model.predict(X_test)\n",
    "o = np.argmax(o, 1)\n",
    "y_test = np.argmax(y_test, 1)\n",
    "sum(np.equal(y_test, o) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 합성곱층의 파라미터\n",
    "- model.add(layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.MaxPooling1D(pool_size=3, strides=2))\n",
    "- 모델에 합성곱층과 풀링충을 추가하고 파라미터를 설정하는 코드 \n",
    "- 간단한 코드이지만 합성 곱층은 LSTM 층이나 완전 연결층에 비해 설정해 주어야 할 부분이 비교적 많은 편\n",
    "1) 필터 수\n",
    "- filters=16\n",
    "- 필터는 합성곱을 수행하는 가중치의 세트\n",
    "- 필터의 개수가 많아질수록 보다 다양한 특징을 추출할 수 있음\n",
    "- 복잡한 데이터를 인식하기 위해서는 다양한 특징을 추출하여 정보량을 늘리는 것은 꼭 필요하기 때문에, 필터 수를 충분히 크게 지정할 필요가 있음\n",
    "- 일반적으로 필터 수가 클수록 올바르게 학습된 네트워크의 성능은 향상되지만, 필터가 올바르게 작동하기 위한 필터 내부의 가중치들이 올바르게 학습되어야 하고, 이런 네트워크 학습이 더욱 어려워짐\n",
    "- 이 문제를 해결하기 위한 기본 전략은 일단 과도하게 많은 수의 필터를 사용하여 학습을 시도한 후, 크기를 조금씩 줄여나가는 것\n",
    "- 다른 전략은 여러 개의 합성곱층을 사용하면서, 필터 수를 조금씩 늘려가는 것\n",
    "- 합성곱층에서 필터 수를 지정하는 코드의 예는 다음과 같음\n",
    "- model.add(layers.Conv1D(filters=8, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "2) 커널 크기\n",
    "- kernel_size = 3\n",
    "- 커널 크기는 가중치 배열의 크기\n",
    "- 커널 크기는 3(2차원인 경우 (3, 3))으로 쓰는 것이 보편적이긴 하지만, 상황에 따라 커널의 크기는 다양하게 바꿀 수 있음\n",
    "- 커널 크기가 클수록 넓은 범위를 포괄할 수 있고, 작을수록 지엽적인 특징을 추출하는 필터가 만들어짐\n",
    "- 일례로 커널 크기를 1로 설정하면, 출력 노드 1개 당 가중치의 개수가 하나로 고정되며, 출력은 단순히 공통의 가중치에 각 입력 노드의 값을 곱한 결과가 됨\n",
    "3) 활성화 함수\n",
    "- activation=\"relu\"\n",
    "- 활성화 함수는 각 층에서 발생하는 출력을 변형시키는 역할을 하며, 하이퍼볼릭 탄젠트 등은 출력을 특정 범위로 조정하기도 함\n",
    "- 합성곱층과 완전 연결층에서 주로 사용하는 활성화 함수인 ReLU는 0보다 작은 출력을 모두 무시하는 역할을 함\n",
    "- 이것은 부분적으로 딥러닝에서 사용되는 대부분의 데이터들이 0 또는 양수라는점에 기인하며, 가중치의 범위를 간접적으로 제한하여, 학습이 보다 쉽고 빠르게 이루어지게 하기 위한 것\n",
    "- ReLU가 출력의 범위를 0 이상의 정수로 조정한다면, 0과 1 사이, -1과 1 사이의 값으로 출력을 조정하는 함수도 있음\n",
    "- ReLU를 사용하면서도 비슷한 효과를 얻는 방법은 별도의 정규화 층을 추가하는 것\n",
    "- 순환 신경망의 활성화 함수를 ReLU로 변경하여 성능을 향상시킬 수 있다는 의견이 있으나, 아직까지 Keras에서는 순환 신경망의 활성화 함수를 변경하는 방법을 별도로 제공하지는 않음\n",
    "- Keras를 사용한 코딩에서 대부분의 활성화 함수들은 Activation 파라미터에 코드를 지정하여 사용할 수 있음\n",
    "- 다만, Leaky ReLU는 고급 활성화 함수로 간주되어 별도의 네트워크층으로 구성하여야 함\n",
    "- 활성화 함수를 별도의 층으로 구성할 때에는 합성곱층 등의 Activation 파라미터를 None으로 지정한 후, 해당 활성화 함수층을 추가\n",
    "- 1차원 합성곱과 Leaky ReLU를 추가하는 코드는 다음과 같음\n",
    "- model.add(layers.Conv1D(filters=16, kernel_size=3, activation=None))\n",
    "- model.add(layers.LeakyReLU())\n",
    "- 딥러닝 모델을 구성함에 있어 활성화 함수는 ReLU를 사용하는 것이 최근의 보편적인 흐름이며, 처음 모델을 구성할 때에는 ReLU를 사용하는 것이 최근의 보편적인 흐름이며, 처음 모델을 구성할 때에는 ReLU를 사용하여 모델을 구성하는 것을 추천\n",
    "- 하지만 다른 딥러닝 파라미터와 마찬가지로, 여기에도 정답은 없고, 활성화 함수를 바꾸어 가며 학습/테스트를 진행하여 성능의 차이를 살펴봄\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "4) 풀링층의 파라미터\n",
    "- 풀링은 노드의 수, 즉 특징의 개수를 줄이기 위한 목적으로 사용\n",
    "- 풀/stride의 크기가 클수록 특징의 개수가 대폭 줄기 때문에, 학습 난이도가 감소하지만, 동시에 정보 손실이 있을 수 있으므로 적절한 균형을 유지하도록하는 것이 중요\n",
    "- 풀/stride의 크기는 네트워크의 깊이(개수)와도 관련이 있음\n",
    "- 풀/stride의 크기가 크면, 네트워크층을 거쳐갈수록 입력 데이터의 크기가 빠르게 줄어들며, 반대의 경우에는 여러 층을 사용하여도 상대적으로 큰 크기를 유지할 수 있기 때문\n",
    "- 예를 들어 16*16 크기의 이미지를 입력으로 받아, 풀/stride의 크기가 2인 풀링층을 세 번만 거치면 4개(2*2)의 노드가 남지만, 만약 크기가 4인 풀링층을 사용한다면 2개 이상의 풀링층은 사용할 수가 없게 됨\n",
    "- 합성곱층과 풀링층이 하나의 세트로 사용되는 점을 생각해 보면, 이것은 네트워크를 구성함에 있어 제법 큰 제약이 됨\n",
    "- 풀링층을 사용하면서도 여러 개의 합성곱층을 사용하는 '딥 네트워크'를 구성하기 위해, 합성곱층과 풀링층을 n:1의 비율로 구성하는 경우도 있음\n",
    "- 합성곱 여러 개를 연속적으로 추가한 후, 풀링층을 한 번만 추가하는 것\n",
    "- 다음은 2:1로 합성곱과 풀링층의 세트를 2개 연속으로 구성하는 코드\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.MaxPooling1D(pool_size=3, strides=2))\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "- model.add(layers.MaxPooling1D(pool_size=3, strides=2))\n",
    "- 이미지 데이터를 인식하는 네트워크에서의 풀링은 MaxPooling이 사용되는데, 이것은 0에 가까울수록 정보가 적어지는 이미지 특성 때문\n",
    "- MaxPooling을 사용하여 해당 풀에서 최댓값을 선택하므로 정보의 손실을 최소화하는 것\n",
    "- 따라서 작은 값이 보다 중요한 정보를 지니고 있다면 MinPooling을, 모든 값이 중요한 상황이라면 성능을 높이기 위해 AveragePooling을 사용하여 테스트를 해 볼 필요가 있음\n",
    "- MinPooling은 Keras에서는 지원하지 않으므로, 필요 시에는 입력 데이터를 반전하여 사용하거나 MinPooling을 자체적으로 구현해야 함\n",
    "- Keras에서 제공하는 주요 풀링층은 Global 풀링층(입력받은 배치에 대해 전체 평균을 계산해 주는 층)과 Local 풀링층(일반적으로 사용)\n",
    "- 풀링층의 또 다른 파라미터는 padding\n",
    "- 이 파라미터의 값이 'valid'이면 패딩 없이 풀링을 수행하고, 'same'인 경우 stride가 1일 때를 기준으로 동일한 크기의 출력을 발생시키도록 제로-패딩을 한 후 결과를 계산\n",
    "- 파라미터를 명시적으로 지정하지 않으면 기본값인 'valid'로 지정됨\n",
    "- padding = \"valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. LSTM층의 파라미터\n",
    "1) 노드 수\n",
    "- LSTM층은 합성곱층에 비해 파라미터의 수가 적어, 파라미터의 지정이 비교적 단순\n",
    "- LSTM에서 필수적인 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
